[Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey

[https://zhuanlan.zhihu.com/p/621188598](https://arxiv.org/abs/1801.00553)

# å¯¹æŠ—æ”»å‡»çš„12ç§æ”»å‡»æ–¹æ³•

## å¯¹åˆ†ç±»ç½‘ç»œçš„æ”»å‡»

| æ–¹æ³•               | é»‘/ç™½ç›’ | å®šå‘/éå®šå‘ | ç‰¹å®š/é€šç”¨ | æ‰°åŠ¨èŒƒæ•°                            | å­¦ä¹ æ–¹å¼ | ç®€ä»‹                                                         |
| ------------------ | ------- | ----------- | --------- | ----------------------------------- | -------- | ------------------------------------------------------------ |
| L-BFGS             | ç™½ç›’    | å®šå‘        | ç‰¹å®š      | $\ell_{\infty}$                     | ä¸€æ­¥     | ä½¿ç”¨L-BFGSç®—æ³•æ¥æœ€å°åŒ–å¯¹æŠ—æ ·æœ¬ä¸åŸå§‹æ ·æœ¬ä¹‹é—´çš„è·ç¦»ï¼ŒåŒæ—¶ç¡®ä¿æ¨¡å‹å¯¹æ ·æœ¬çš„åˆ†ç±»å‘ç”Ÿå˜åŒ– |
| FGSM               | ç™½ç›’    | å®šå‘        | ç‰¹å®š      | $\ell_{\infty}$                     | ä¸€æ­¥     | é€šè¿‡æ²¿ç€æŸå¤±å‡½æ•°æ¢¯åº¦çš„æ–¹å‘æ–½åŠ ä¸€æ­¥æ‰°åŠ¨æ¥å¿«é€Ÿç”Ÿæˆå¯¹æŠ—æ ·æœ¬     |
| BIM & ILCM         | ç™½ç›’    | éå®šå‘      | ç‰¹å®š      | $\ell_{\infty}$                     | è¿­ä»£     | åŸºäºFGSMçš„è¿­ä»£ç‰ˆæœ¬ï¼Œé€šè¿‡å¤šæ¬¡è¿­ä»£é€æ­¥å¢åŠ æ‰°åŠ¨ä»¥æé«˜å¯¹æŠ—æ ·æœ¬çš„æˆåŠŸç‡ |
| JSMA               | ç™½ç›’    | å®šå‘        | ç‰¹å®š      | $\ell_0$                            | è¿­ä»£     | åˆ©ç”¨è¾“å…¥ç‰¹å¾ç›¸å¯¹äºè¾“å‡ºçš„å˜åŒ–æ•æ„Ÿæ€§æ¥é€‰æ‹©å¹¶ä¿®æ”¹å°‘é‡å…³é”®åƒç´ ç‚¹ï¼Œä»¥è¯¯å¯¼æ¨¡å‹ |
| One-pixel          | é»‘ç›’    | éå®šå‘      | ç‰¹å®š      | $\ell_0$                            | è¿­ä»£     | ä»…é€šè¿‡æ”¹å˜å›¾åƒä¸­çš„ä¸€ä¸ªæˆ–å°‘æ•°å‡ ä¸ªåƒç´ ç‚¹æ¥äº§ç”Ÿå¯¹æŠ—æ ·æœ¬         |
| C&W attacks        | ç™½ç›’    | å®šå‘        | ç‰¹å®š      | $\ell_0$, $\ell_2$, $\ell_{\infty}$ | è¿­ä»£     | èƒ½å¤Ÿé’ˆå¯¹å¤šç§èŒƒæ•°çº¦æŸç”Ÿæˆéš¾ä»¥å¯Ÿè§‰çš„å¯¹æŠ—æ ·æœ¬                   |
| DeepFool           | ç™½ç›’    | éå®šå‘      | ç‰¹å®š      | $\ell_2$, $\ell_{\infty}$           | è¿­ä»£     | é€šè¿‡è¿­ä»£åœ°è®¡ç®—æ¨¡å‹å†³ç­–è¾¹ç•Œçš„æœ€è¿‘ç‚¹æ¥ç”Ÿæˆå¯¹æŠ—æ ·æœ¬             |
| Uni. perturbations | ç™½ç›’    | éå®šå‘      | é€šç”¨      | $\ell_2$, $\ell_{\infty}$           | è¿­ä»£     | å¯»æ‰¾é€šç”¨çš„æ‰°åŠ¨ï¼Œå¯ä»¥åº”ç”¨äºä¸åŒçš„å¹²å‡€æ ·æœ¬ä¸Šï¼Œä½¿å¾—å®ƒä»¬éƒ½è¢«è¯¯åˆ†ç±» |
| UPSET              | é»‘ç›’    | å®šå‘        | é€šç”¨      | $\ell_{\infty}$                     | è¿­ä»£     | ä¸€ç§é»‘ç›’æ”»å‡»æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ç‰©ç†ä¸–ç•Œä¸­ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œå¹¶ä¸”è¿™äº›æ ·æœ¬å¯¹äºä¸åŒæ¨¡å‹å…·æœ‰ä¸€å®šçš„é€šç”¨æ€§ |
| ANGRI              | é»‘ç›’    | å®šå‘        | ç‰¹å®š      | $\ell_{\infty}$                     | è¿­ä»£     | é€šè¿‡éšæœºåˆå§‹åŒ–ç»“åˆæ¢¯åº¦ä¿¡æ¯æ¥ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œæ˜¯ä¸€ç§é»‘ç›’å®šå‘æ”»å‡»æ–¹æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºä¸çŸ¥é“æ¨¡å‹å†…éƒ¨ç»“æ„çš„æƒ…å†µ |
| Houdini            | é»‘ç›’    | å®šå‘        | ç‰¹å®š      | $\ell_2$, $\ell_{\infty}$           | è¿­ä»£     | é€šè¿‡ç”Ÿæˆç‰¹å®šäºä»»åŠ¡æŸå¤±å‡½æ•°çš„å¯¹æŠ—æ ·æœ¬å®ç°å¯¹æŠ—æ”»å‡»ï¼Œé™¤äº†å›¾åƒåˆ†ç±»ç½‘ç»œï¼Œè¯¥ç®—æ³•è¿˜å¯ä»¥ç”¨äºæ¬ºéª—è¯­éŸ³è¯†åˆ«ç½‘ç»œã€‚ |
| ATNs               | ç™½ç›’    | å®šå‘        | ç‰¹å®š      | $\ell_{\infty}$                     | è¿­ä»£     | é€šè¿‡è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œæ¥å­¦ä¹ å¦‚ä½•å°†æ­£å¸¸æ ·æœ¬è½¬æ¢ä¸ºå¯¹æŠ—æ ·æœ¬       |

### Box-constrained L-BFGS

å¯»æ‰¾æœ€å°çš„æŸå¤±å‡½æ•°æ·»åŠ é¡¹ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œåšå‡ºè¯¯åˆ†ç±»ï¼Œå°†é—®é¢˜è½¬åŒ–æˆå‡¸ä¼˜åŒ–è¿‡ç¨‹ã€‚

ç›®æ ‡å‡½æ•°ï¼š$ \minâ¡ câ‹…\Vert\delta\Vert_2+L(I_c+\delta), s.t. I_c+\deltaâˆˆ[0,1]^m$

1. $I_c$è¡¨ç¤ºåŸå§‹å›¾åƒ
2. $\delta$è¡¨ç¤ºå¯¹åŸå§‹å›¾åƒ$I_c$çš„æ‰°åŠ¨
3. $c$æ˜¯ä¸€ä¸ªæ­£åˆ™åŒ–å‚æ•°ï¼Œç”¨äºå¹³è¡¡æ‰°åŠ¨å¤§å°å’ŒæŸå¤±å‡½æ•°ä¹‹é—´çš„å…³ç³»
4. $\Vert\delta\Vert_2$æ˜¯$\delta$çš„$L_2$èŒƒæ•°ï¼Œå³$\sum_{i=1}^{m} \delta_i^2$
5. $L(I_c+\delta)$ä¸ºæ‰°åŠ¨åå›¾åƒä¸Šçš„æŸå¤±
6. $I_c+\delta \left[0,1\right]^m\in $ç¡®ä¿æ‰°åŠ¨åçš„å›¾åƒä»ç„¶åœ¨åˆæ³•çš„åƒç´ å€¼èŒƒå›´å†…

è¿‡ç¨‹ï¼š

1. ç›®æ ‡å‡½æ•°çš„æ¢¯åº¦è®¡ç®—ï¼š
   - éœ€è¦è®¡ç®— $F(\delta)$ å¯¹ $\delta$ çš„æ¢¯åº¦ï¼š$ âˆ‡_{\delta}F(Î´)=câ‹…âˆ‡_{\delta}\VertÎ´\Vert_2+âˆ‡_{\delta} L(I_c+\delta)$.
   - $L(I_c + \delta)$ é€šå¸¸é€šè¿‡åå‘ä¼ æ’­è®¡ç®—åˆ†ç±»å™¨çš„æ¢¯åº¦ã€‚
2. Box æŠ•å½±æ“ä½œï¼š
   - L-BFGS ä¼šç”Ÿæˆä¸€ç³»åˆ—å€™é€‰æ‰°åŠ¨ $\delta^{(k)}$ã€‚
   - å¦‚æœ $\delta^{(k)}$ è¶…å‡ºè¾¹ç•Œï¼Œåˆ™ä½¿ç”¨æŠ•å½±æ“ä½œå°†å…¶æŠ•å½±å›çº¦æŸèŒƒå›´ï¼š $\delta^{(k)} = \min(\max(\delta^{(k)}, -I_c), 1 - I_c)$
3. ä¼˜åŒ–è¿­ä»£ï¼š
   - ä½¿ç”¨ L-BFGS ç®—æ³•è¿­ä»£æ›´æ–° $\delta^{(k)}$ï¼Œç›´åˆ°ç›®æ ‡å‡½æ•° $F(\delta)$ æ”¶æ•›ã€‚
   - åœæ­¢æ¡ä»¶åŒ…æ‹¬ï¼š
     - æŸå¤±å‡½æ•° $F(\delta)$ ä¸å†æ˜¾è‘—å˜åŒ–ã€‚
     - $\delta^{(k)}$ å¯¼è‡´åˆ†ç±»å™¨çš„è¾“å‡ºè¾¾åˆ°ç›®æ ‡ç±»åˆ«ã€‚
4. åŠ¨æ€è°ƒæ•´æ­£åˆ™åŒ–å‚æ•° $c$
   1. å¼€å§‹ç”¨åˆå§‹å€¼ $c$ï¼Œè¿è¡ŒL-BFGSä¼˜åŒ–ã€‚
   2. æ£€æŸ¥ä¼˜åŒ–åçš„æ‰°åŠ¨ $\delta$ æ˜¯å¦èƒ½æ»¡è¶³åˆ†ç±»å™¨çš„ç›®æ ‡ç»“æœï¼ˆå³ï¼Œ$I_c + \delta$ çš„é¢„æµ‹ä¸ºç›®æ ‡ç±»åˆ« $y_t$ï¼‰ã€‚
      1. å¦‚æœæˆåŠŸï¼Œå‡å° $c$ï¼Œè¿›ä¸€æ­¥æœ€å°åŒ–æ‰°åŠ¨ã€‚
      2. å¦‚æœå¤±è´¥ï¼Œå¢å¤§ $c$ï¼Œå…è®¸æ›´å¤§çš„æ‰°åŠ¨ã€‚
   3. é‡å¤äºŒåˆ†æœç´¢ï¼Œç›´åˆ°æ‰¾åˆ°æœ€å°çš„ $c > 0$ã€‚

#### ä»£ç å®ç°

```python
import numpy as np
from scipy.optimize import minimize

# æŸå¤±å‡½æ•°çš„å°è£…
def objective_function(delta, I_c, c, target_label, classifier):
    perturbed_image = I_c + delta
    # åˆ†ç±»å™¨çš„æŸå¤± (å¦‚äº¤å‰ç†µæŸå¤±)
    loss = classifier.loss(perturbed_image, target_label)
    # æ­£åˆ™é¡¹
    regularization = c * np.linalg.norm(delta, ord=2)
    return loss + regularization

# æ¢¯åº¦è®¡ç®—
def gradient(delta, I_c, c, target_label, classifier):
    perturbed_image = I_c + delta
    # è®¡ç®—æŸå¤±çš„æ¢¯åº¦
    loss_grad = classifier.loss_gradient(perturbed_image, target_label)
    # æ­£åˆ™é¡¹çš„æ¢¯åº¦
    regularization_grad = 2 * c * delta
    return loss_grad + regularization_grad

# æŠ•å½±æ“ä½œ
def project(delta, I_c):
    return np.clip(delta, -I_c, 1 - I_c)

# ä¼˜åŒ–æµç¨‹
def box_constrained_l_bfgs(I_c, target_label, classifier, c_init, c_min, c_max, tol):
    c = c_init
    while c_max - c_min > tol:
        # ä¼˜åŒ–é—®é¢˜å®šä¹‰
        result = minimize(
            fun=objective_function,
            x0=np.zeros_like(I_c),  # åˆå§‹æ‰°åŠ¨
            args=(I_c, c, target_label, classifier),
            jac=gradient,
            bounds=[(-I_c[i], 1 - I_c[i]) for i in range(I_c.size)],
            method='L-BFGS-B'
        )
        delta = result.x.reshape(I_c.shape)
        # æ£€æŸ¥åˆ†ç±»å™¨ç»“æœ
        if classifier.predict(I_c + delta) == target_label:
            c_max = c  # å‡å°æ­£åˆ™åŒ–å‚æ•°
        else:
            c_min = c  # å¢å¤§æ­£åˆ™åŒ–å‚æ•°
        c = (c_min + c_max) / 2
    return delta
```



###  Fast Gradient Sign Method (FGSM)

é€šè¿‡ç”¨è¯†åˆ«æ¦‚ç‡æœ€å°çš„ç±»åˆ«ï¼ˆç›®æ ‡ç±»åˆ«ï¼‰ä»£æ›¿å¯¹æŠ—æ‰°åŠ¨ä¸­çš„ç±»åˆ«å˜é‡ï¼Œå†å°†åŸå§‹å›¾åƒå‡å»è¯¥æ‰°åŠ¨ï¼ŒåŸå§‹å›¾åƒå°±å˜æˆäº†å¯¹æŠ—æ ·æœ¬ï¼Œå¹¶èƒ½è¾“å‡ºç›®æ ‡ç±»åˆ«ã€‚

ç»™å®šä¸€ä¸ªåŸå§‹å›¾åƒ$I_c$ å’Œä¸€ä¸ªå°çš„æ ‡é‡å€¼$\epsilon$ï¼ŒFGSM ç”Ÿæˆæ‰°åŠ¨çš„è¿‡ç¨‹ï¼š$\delta = \epsilon \cdot \mathrm{sign}(\nabla_{I_c } L(I_c,y))$

1. $\nabla_{I_c } J(I_c,y)$æ˜¯æŸå¤±å‡½æ•°$L$å¯¹è¾“å…¥å›¾åƒ$I_c$ä¸ç›®æ ‡æ ‡ç­¾$y$çš„æ¢¯åº¦
2. $\mathrm{sign}(\cdot)$å‡½æ•°å°†æ¢¯åº¦ä¸­çš„æ¯ä¸ªå…ƒç´ è½¬æ¢ä¸ºå…¶ç¬¦å·ï¼ˆæ­£ã€è´Ÿæˆ–é›¶ï¼‰
   1. ç¬¦å·å‡½æ•°åªæ”¹å˜äº†æ¯ä¸ªåƒç´ çš„æ­£è´Ÿæ–¹å‘ï¼Œè€Œä¸æ”¹å˜å…¶ç»å¯¹å€¼
3. $\epsilon$ç”¨äºæ§åˆ¶æ‰°åŠ¨çš„å¤§å°

ä¸¤ç§æ”»å‡»ç›®æ ‡ï¼š

1. éç›®æ ‡æ”»å‡» (Untargeted Attack)
   1. ç›®æ ‡: è®©æ¨¡å‹çš„è¾“å‡ºåç¦»åŸå§‹é¢„æµ‹ç»“æœï¼Œè¾¾åˆ°é”™è¯¯åˆ†ç±»
   2. å®ç°: åœ¨éç›®æ ‡æ”»å‡»ä¸­ï¼Œè®¡ç®—æŸå¤±ä½¿ç”¨çš„æ˜¯æ¨¡å‹æ­£å¸¸é¢„æµ‹çš„æ ‡ç­¾ï¼Œå¹¶ä¸”æ‰°åŠ¨æ²¿ç€**æŸå¤±å¢åŠ çš„æ–¹å‘**ï¼ˆå³æ¢¯åº¦çš„æ­£æ–¹å‘ï¼‰æ–½åŠ 
   3. è§£é‡Š: è¿™é‡Œæ˜¯åŠ æ¢¯åº¦ï¼Œå› ä¸ºå¢åŠ æ¢¯åº¦æ–¹å‘çš„æ‰°åŠ¨ä¼šè®©æ¨¡å‹å¯¹æ­£ç¡®ç±»åˆ«çš„ç½®ä¿¡åº¦é™ä½ï¼Œä»è€Œæ›´å®¹æ˜“é”™è¯¯åˆ†ç±»
2. ç›®æ ‡æ”»å‡» (Targeted Attack)
   1. ç›®æ ‡: è®©æ¨¡å‹çš„è¾“å‡ºå˜ä¸ºç›®æ ‡ç±»åˆ«ï¼ˆæ”»å‡»è€…æŒ‡å®šçš„ç±»åˆ«ï¼‰
   2. å®ç°: åœ¨ç›®æ ‡æ”»å‡»ä¸­ï¼Œè®¡ç®—æŸå¤±ä½¿ç”¨çš„æ˜¯ç›®æ ‡æ ‡ç­¾ï¼Œå¹¶ä¸”æ‰°åŠ¨æ²¿ç€**æŸå¤±å‡å°‘çš„æ–¹å‘**ï¼ˆå³æ¢¯åº¦çš„åæ–¹å‘ï¼‰æ–½åŠ 
   3. è§£é‡Š: è¿™é‡Œæ˜¯å‡æ¢¯åº¦ï¼Œå› ä¸ºå‡å°‘æŸå¤±æ„å‘³ç€å¼•å¯¼æ¨¡å‹çš„é¢„æµ‹ç»“æœæœç›®æ ‡ç±»åˆ«é è¿‘

#### ä»£ç å®ç°

```python
import torch
import torch.nn.functional as F

def fgsm_attack(model, input_image, target_label, epsilon):
    """
    Args:
        model (torch.nn.Module): ç”¨äºåˆ†ç±»çš„æ¨¡å‹ã€‚
        input_image (torch.Tensor): è¾“å…¥çš„åŸå§‹å›¾åƒ (batch_size, channels, height, width)ï¼Œ
                                    å€¼é€šå¸¸åœ¨ [0, 1]ã€‚
        target_label (torch.Tensor): ç›®æ ‡ç±»åˆ«çš„æ ‡ç­¾ (batch_size,)ã€‚
        epsilon (float): æ§åˆ¶æ‰°åŠ¨å¤§å°çš„å‚æ•°ã€‚
    
    Returns:
        torch.Tensor: æ·»åŠ æ‰°åŠ¨åçš„å¯¹æŠ—æ ·æœ¬ã€‚
    """
    # ç¡®ä¿è¾“å…¥å›¾åƒéœ€è¦æ¢¯åº¦è®¡ç®—
    input_image.requires_grad = True
    # å°†å›¾åƒä¼ é€’ç»™æ¨¡å‹ï¼Œè®¡ç®—æŸå¤±
    output = model(input_image)  # æ¨¡å‹é¢„æµ‹ logits
    loss = F.cross_entropy(output, target_label)  # ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°
    # è®¡ç®—è¾“å…¥å›¾åƒçš„æ¢¯åº¦
    model.zero_grad()  # æ¸…é™¤å…ˆå‰çš„æ¢¯åº¦
    loss.backward()    # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
    # ç”Ÿæˆæ‰°åŠ¨ï¼šè®¡ç®—æ¢¯åº¦çš„ç¬¦å·ï¼Œç„¶åä¹˜ä»¥ epsilon
    grad_sign = input_image.grad.data.sign()
    perturbation = epsilon * grad_sign
    # æ·»åŠ æ‰°åŠ¨ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
    adversarial_image = input_image + perturbation
    # ç¡®ä¿å¯¹æŠ—æ ·æœ¬ä»ç„¶åœ¨åˆæ³•åƒç´ èŒƒå›´ [0, 1]
    adversarial_image = torch.clamp(adversarial_image, 0, 1)
    return adversarial_image
```



### Basic & Least-Likely-Class Iterative Methods

#### Basic Iterative Method (BIM)

è¿­ä»£åœ°é‡‡å–å¤šä¸ªå°æ­¥éª¤ï¼ŒåŒæ—¶åœ¨æ¯ä¸ªæ­¥éª¤åè°ƒæ•´æ–¹å‘ã€‚

$I_{\rho}^{i+1} = \mathrm{Clip}_{\epsilon}\left\{I_{\rho}^{i} + \alpha \mathrm{sign}(\nabla L(\theta, I_{\rho}^{i},y))\right\}$

1. $I_{\rho}^{i}$è¡¨ç¤ºç¬¬$i$è½®è¿­ä»£åçš„å¯¹æŠ—æ ·æœ¬
2. $y$ä¸ºæ­£å¸¸é¢„æµ‹çš„æ ‡ç­¾

#### Iterative Least-likely Class Method (ILCM)

$I_{\rho}^{i+1} = \mathrm{Clip}_{\epsilon}\left\{I_{\rho}^{i} - \alpha \mathrm{sign}(\nabla L(\theta, I_{\rho}^{i},y_{target}))\right\}$

ä»è€Œæ”¹å˜å¯¹æŠ—æ ·æœ¬çš„åˆ†ç±»ä¸ºç›®æ ‡æ ‡ç­¾

### Jacobian-based Saliency Map Attack (JSMA)

åˆ©ç”¨ç½‘ç»œè¾“å‡ºå±‚å…³äºè¾“å…¥å›¾åƒçš„æ¢¯åº¦ä¿¡æ¯æ¥æ„å»ºä¸€ä¸ªæ˜¾è‘—æ€§å›¾(saliency map)ã€‚

è¿™ä¸ªæ˜¾è‘—æ€§å›¾ç”¨æ¥æŒ‡ç¤ºå“ªäº›åƒç´ ç‚¹çš„å˜åŒ–æœ€æœ‰å¯èƒ½å¯¼è‡´æ¨¡å‹å°†å›¾åƒåˆ†ç±»ä¸ºæ”»å‡»è€…æŒ‡å®šçš„ç›®æ ‡ç±»åˆ«ï¼Œè€ŒéåŸå§‹ç±»åˆ«ã€‚

é€šè¿‡é€‰æ‹©æœ€èƒ½æœ‰æ•ˆæ¬ºéª—ç½‘ç»œçš„åƒç´ å¹¶å¯¹å…¶è¿›è¡Œæ›´æ”¹ï¼Œé‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°æ”¹å˜å¯¹æŠ—æ€§å›¾åƒä¸­å…è®¸çš„æœ€å¤§åƒç´ æ•°æˆ–è€…æ¬ºéª—æˆåŠŸã€‚

#### è®¡ç®—saliency mapè¿‡ç¨‹

1. é¦–å…ˆæ˜ç¡®æ”»å‡»çš„ç›®æ ‡ç±»åˆ«$t$å’ŒåŸå§‹ï¼ˆæ­£ç¡®ï¼‰ç±»åˆ«$y$ã€‚
2. å¯¹äºç»™å®šçš„è¾“å…¥å›¾åƒ$X$ï¼Œè®¡ç®—æ¨¡å‹è¾“å‡ºå¯¹è¾“å…¥å›¾åƒæ¯ä¸ªåƒç´ çš„åå¯¼æ•°ï¼Œå³å½¢æˆé›…å¯æ¯”çŸ©é˜µ$J$ã€‚é›…å¯æ¯”çŸ©é˜µçš„æ¯ä¸€é¡¹$J_{ij}$è¡¨ç¤ºç¬¬$i$ä¸ªè¾“å‡ºèŠ‚ç‚¹ç›¸å¯¹äºç¬¬$j$ä¸ªè¾“å…¥åƒç´ çš„å˜åŒ–ç‡ã€‚
   1. å¦‚æœæ¨¡å‹æœ‰$n$ä¸ªè¾“å‡ºèŠ‚ç‚¹ï¼ˆä¾‹å¦‚åˆ†ç±»é—®é¢˜ä¸­çš„ç±»åˆ«æ•°é‡ï¼‰ï¼Œè€Œè¾“å…¥å›¾åƒæœ‰$m$ä¸ªåƒç´ ï¼Œåˆ™é›…å¯æ¯”çŸ©é˜µ$J$çš„ç»´åº¦ä¸º$nÃ—m$ã€‚
3. ä¸ºäº†æ„é€ æ˜¾è‘—æ€§å›¾ï¼Œéœ€è¦å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è¡¡é‡æ”¹å˜ç‰¹å®šåƒç´ å¯¹ç›®æ ‡ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡çš„å½±å“ã€‚é€šå¸¸é‡‡ç”¨çš„æ˜¯ä»¥ä¸‹ä¸¤ç§å½¢å¼ä¹‹ä¸€ï¼š
   - ç›´æ¥ä½¿ç”¨é›…å¯æ¯”çŸ©é˜µä¸­å¯¹åº”äºç›®æ ‡ç±»åˆ«å’Œå½“å‰åƒç´ çš„å…ƒç´ ã€‚
   - ä½¿ç”¨ä¸€ç§ç»„åˆå½¢å¼ï¼Œæ¯”å¦‚è€ƒè™‘åŒæ—¶å¢åŠ ç›®æ ‡ç±»åˆ«$t$çš„æ¦‚ç‡å’Œå‡å°‘åŸå§‹ç±»åˆ«$y$çš„æ¦‚ç‡ï¼Œè¿™å¯ä»¥é€šè¿‡è®¡ç®—$J_{tj}âˆ’J_{yj}$æ¥å®ç°
4. æ ¹æ®ä¸Šè¿°åº¦é‡æ–¹æ³•ï¼Œä¸ºæ¯ä¸ªåƒç´ åˆ†é…ä¸€ä¸ªå€¼ï¼Œå½¢æˆæ˜¾è‘—æ€§å›¾ã€‚
   1. æ˜¾è‘—æ€§å›¾ä¸Šçš„æ¯ä¸€ä¸ªå€¼åæ˜ äº†ä¿®æ”¹è¯¥åƒç´ åå¯èƒ½å¯¹ç›®æ ‡ç±»åˆ«é¢„æµ‹æ¦‚ç‡å¢ç›Šçš„å¤§å°ã€‚
   2. è¾ƒå¤§çš„æ­£å€¼è¡¨ç¤ºä¿®æ”¹è¯¥åƒç´ æœ‰åŠ©äºå°†å›¾åƒåˆ†ç±»ä¸ºç›®æ ‡ç±»åˆ«ï¼›
   3. è¾ƒå°æˆ–è´Ÿå€¼åˆ™æ„å‘³ç€ä¿®æ”¹è¯¥åƒç´ å¯èƒ½ä¸åˆ©äºè¾¾åˆ°ç›®çš„ã€‚
5. åœ¨å®é™…æ“ä½œæ—¶ï¼Œè¿˜éœ€è¦è€ƒè™‘ä¸€äº›é¢å¤–çš„çº¦æŸæ¡ä»¶ï¼Œæ¯”å¦‚ç¡®ä¿ä¿®æ”¹åçš„åƒç´ å€¼ä»åœ¨åˆç†èŒƒå›´å†…ï¼Œä»¥åŠé¿å…é€‰æ‹©é‚£äº›å·²ç»ä¿®æ”¹è¿‡çš„åƒç´ ç­‰ã€‚

#### ä»£ç å®ç°

```python
import torch
import torch.nn as nn
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

def compute_jacobian(model, image):
    """
    å‚æ•°:
        model: PyTorchæ¨¡å‹
        image: è¾“å…¥å›¾åƒ (Variable)
    """
    # è®¾ç½®å›¾åƒä¸ºå˜é‡å¹¶è·Ÿè¸ªæ¢¯åº¦
    if not isinstance(image, torch.Tensor):
        image = torch.tensor(image, requires_grad=True, dtype=torch.float32)
    else:
        image.requires_grad_(True)
    # è·å–è¾“å‡ºå±‚å¼ é‡
    output = model(image.unsqueeze(0))  # å¢åŠ batchç»´åº¦
    num_classes = output.size(1)
    jacobian = torch.zeros((num_classes, *image.shape))
    for i in range(num_classes):
        # æ¸…ç©ºæ¢¯åº¦
        model.zero_grad()
        # åªé’ˆå¯¹ç¬¬iä¸ªç±»åˆ«çš„è¾“å‡ºè®¡ç®—æ¢¯åº¦
        grad_output = torch.zeros_like(output)
        grad_output[0, i] = 1  # å‡è®¾batch sizeä¸º1
        # è®¡ç®—æ¢¯åº¦
        output.backward(gradient=grad_output, retain_graph=True)
        # å°†æ¢¯åº¦å¤åˆ¶åˆ°é›…å¯æ¯”çŸ©é˜µ
        jacobian[i] = image.grad.data.clone()
    return jacobian.numpy()

def compute_saliency_map(jacobian, target_class, original_class):
    """
    å‚æ•°:
        jacobian: é›…å¯æ¯”çŸ©é˜µ
        target_class: ç›®æ ‡ç±»åˆ«ç´¢å¼•
        original_class: åŸå§‹ç±»åˆ«ç´¢å¼•
    """
    # è®¡ç®—ç›®æ ‡ç±»åˆ«çš„æ¢¯åº¦
    target_gradient = jacobian[target_class]
    # è®¡ç®—åŸå§‹ç±»åˆ«çš„æ¢¯åº¦
    original_gradient = jacobian[original_class]
    # è®¡ç®—æ˜¾è‘—æ€§å›¾
    saliency_map = target_gradient - original_gradient
    # ç¡®ä¿æ­£å€¼
    saliency_map = np.maximum(saliency_map, 0)
    return saliency_map

def jsma_attack(model, image, target_class, original_class, max_perturbed_pixels=50):
    """
    å‚æ•°:
        model: PyTorchæ¨¡å‹
        image: è¾“å…¥å›¾åƒ (numpy array)
        target_class: ç›®æ ‡ç±»åˆ«ç´¢å¼•
        original_class: åŸå§‹ç±»åˆ«ç´¢å¼•
        max_perturbed_pixels: æœ€å¤šå…è®¸ä¿®æ”¹çš„åƒç´ æ•°é‡
    """
    adversarial_image = image.copy()
    perturbed_pixels = 0
    while perturbed_pixels < max_perturbed_pixels:
        # è®¡ç®—å½“å‰å¯¹æŠ—æ ·æœ¬çš„é›…å¯æ¯”çŸ©é˜µ
        jacobian = compute_jacobian(model, torch.tensor(adversarial_image))
        # è®¡ç®—æ˜¾è‘—æ€§å›¾
        saliency_map = compute_saliency_map(jacobian, target_class, original_class)
        # æ‰¾åˆ°æ˜¾è‘—æ€§å›¾ä¸­æœ€å¤§å€¼çš„ä½ç½®
        max_index = np.unravel_index(np.argmax(saliency_map), saliency_map.shape)
        # ä¿®æ”¹è¯¥ä½ç½®çš„åƒç´ å€¼
        # è¿™é‡Œç®€å•åœ°å¢åŠ ä¸€ä¸ªå°æ‰°åŠ¨ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„ç­–ç•¥
        delta = 0.01  # å°æ‰°åŠ¨å¤§å°
        adversarial_image[max_index] += delta
        # æ£€æŸ¥æ˜¯å¦å·²ç»æˆåŠŸæ¬ºéª—äº†æ¨¡å‹
        with torch.no_grad():
            output = model(torch.tensor(adversarial_image).unsqueeze(0))
            predicted_class = torch.argmax(output, dim=1).item()
        if predicted_class == target_class:
            print(f"Attack succeeded after modifying {perturbed_pixels + 1} pixels.")
            break
        perturbed_pixels += 1
    return adversarial_image

# åŠ è½½ä½ çš„æ¨¡å‹
model = ...  # è¿™é‡ŒåŠ è½½ä½ çš„æ¨¡å‹
model.eval()  # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
# å‡†å¤‡è¾“å…¥å›¾åƒ
image_path = 'path_to_your_image.jpg'
image = Image.open(image_path).convert('RGB')
image = np.array(image, dtype=np.float32) / 255.0  # å½’ä¸€åŒ–
image = image.transpose(2, 0, 1)  # è½¬æ¢ä¸ºC, H, Wæ ¼å¼
# è®¾ç½®ç›®æ ‡ç±»åˆ«å’ŒåŸå§‹ç±»åˆ«
target_class = ...  # ç›®æ ‡ç±»åˆ«ç´¢å¼•
original_class = ...  # åŸå§‹ç±»åˆ«ç´¢å¼•
# æ‰§è¡ŒJSMAæ”»å‡»
adversarial_image = jsma_attack(model, image, target_class, original_class)
```

### One Pixel Attack

é€šè¿‡ä¿®æ”¹å›¾åƒä¸­çš„å•ä¸ªåƒç´ æ¥æ¬ºéª—æ·±åº¦å­¦ä¹ åˆ†ç±»å™¨ã€‚

å…·ä½“ä½¿ç”¨äº†å·®åˆ†è¿›åŒ–ï¼ˆDifferential Evolutionï¼‰ç®—æ³•æ¥å¯»æ‰¾èƒ½å¤Ÿæœ€å¤§åŒ–æ”¹å˜åˆ†ç±»å™¨è¾“å‡ºçš„åƒç´ ä½ç½®å’Œé¢œè‰²å€¼ã€‚

**è¿™ç§æ”»å‡»æ–¹æ³•åªéœ€è¦çŸ¥é“ç›®æ ‡æ¨¡å‹å¯¹è¾“å…¥å›¾åƒçš„é¢„æµ‹æ¦‚ç‡ï¼Œè€Œä¸éœ€è¦äº†è§£æ¨¡å‹å†…éƒ¨çš„å·¥ä½œæœºåˆ¶ï¼Œå› æ­¤æ˜¯ä¸€ç§é»‘ç›’æ”»å‡»æ–¹æ³•ã€‚**

è¿‡ç¨‹ï¼š

1. é¦–å…ˆï¼Œå¯¹äºä¸€ä¸ªå¹²å‡€çš„å›¾åƒ$I_c$ï¼Œåˆ›å»ºä¸€ç»„ 400 ä¸ªå‘é‡ï¼Œæ¯ä¸ªå‘é‡åŒ…å«äº”ä¸ªå…ƒç´ ï¼ˆxy åæ ‡å’Œ RGB é¢œè‰²å€¼ï¼‰ï¼Œè¡¨ç¤ºä¸€ä¸ªå€™é€‰åƒç´ çš„ä½ç½®å’Œé¢œè‰²ã€‚
2. éšæœºä¿®æ”¹è¿™äº›å‘é‡ä»¥ç”Ÿæˆæ–°çš„â€œå­ä»£â€å‘é‡ã€‚
   1. æ¯ä¸ªå­ä»£å‘é‡ä¸å®ƒçš„çˆ¶ä»£ç«äº‰ï¼Œä»¥å†³å®šè°ä¼šåœ¨ä¸‹ä¸€æ¬¡è¿­ä»£ä¸­å­˜æ´»ä¸‹æ¥ã€‚
   2. è¿™é‡Œçš„é€‚åº”åº¦å‡½æ•°æ˜¯åŸºäºç¥ç»ç½‘ç»œå¯¹ä¿®æ”¹åçš„å›¾åƒè¿›è¡Œåˆ†ç±»æ—¶çš„ç½®ä¿¡åº¦â€”â€”ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªåƒç´ æ›´æ”¹ï¼Œä½¿å¾—ç½‘ç»œå¯¹é”™è¯¯ç±»åˆ«çš„ç½®ä¿¡åº¦æœ€é«˜ã€‚
3. ç»è¿‡å¤šè½®è¿­ä»£åï¼Œæœ€ç»ˆä¿ç•™ä¸‹æ¥çš„å­ä»£å‘é‡è¢«ç”¨æ¥ä¿®æ”¹åŸå§‹å›¾åƒä¸­çš„ç›¸åº”åƒç´ ã€‚è¿™ä¸ªä¿®æ”¹åçš„å›¾åƒå°±æ˜¯å¯¹æŠ—æ ·æœ¬ã€‚

### Carlini and Wagner Attacks (C&W)

[C&W attack - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/259727988)

[åŸºäºä¼˜åŒ–çš„æ”»å‡»â€”â€”CW](https://www.cnblogs.com/tangweijqxx/p/10627360.html)

C&W æ”»å‡»èƒ½å¤Ÿç»•è¿‡é˜²å¾¡è’¸é¦ï¼ˆdefensive distillationï¼Œé€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¸©åº¦å‚æ•°è°ƒæ•´æ¥å¹³æ»‘æ¨¡å‹è¾“å‡ºåˆ†å¸ƒçš„æŠ€æœ¯ï¼Œç”¨äºæé«˜æ¨¡å‹æŠµæŠ—å¯¹æŠ—æ ·æœ¬çš„èƒ½åŠ›ï¼‰ã€‚

C&W æ”»å‡»å±•ç¤ºäº†å¯¹æŠ—æ ·æœ¬çš„è¿ç§»æ€§ï¼Œå³åœ¨ä¸€ä¸ªæœªåŠ å›ºçš„æ¨¡å‹ä¸Šç”Ÿæˆçš„å¯¹æŠ—æ ·æœ¬å¯ä»¥åœ¨å¦ä¸€ä¸ªåŠ å›ºåçš„æ¨¡å‹ä¸ŠåŒæ ·æœ‰æ•ˆã€‚

1. C&Wæ”»å‡»å¯»æ‰¾å¯¹æŠ—æ ·æœ¬çš„é—®é¢˜å®šä¹‰ï¼š$\mathrm{minimize}D(x, x+\delta),\ s.t.\ C(x+\delta)=t, (x+\delta) \in \left[0,1\right]^n$
2. ä½¿é—®é¢˜æ˜“è¢«æ¢¯åº¦ç®—æ³•æ±‚è§£ï¼š
   1. å°†$C(x+\delta)=t$çº¦æŸå˜ä¸ºç­‰ä»·çš„objection functionï¼Œä¸$D(x, x+\delta)$ä¸€èµ·ä¼˜åŒ–
      1. å»ºç«‹$f$ï¼Œä½¿å¾—$C(x+\delta)=t$æˆç«‹å½“ä¸”ä»…å½“$f(x+\delta) \leq 0$
      2. é—®é¢˜å®šä¹‰ï¼š$\mathrm{minimize}(\Vert\delta\Vert_p + c\cdot f(x+\delta)), s.t. (x+\delta) \in \left[0,1\right]^n$
   2. å°†$(x+\delta) \in \left[0,1\right]^n$çº¦æŸåœ¨ç”Ÿæˆå¯¹æŠ—æ‰°åŠ¨$\delta$ä¸­å®ç°
      1. ä½œè€…æå‡ºäº†ä¸‰ç§åŸºäºè·ç¦»åº¦é‡çš„æ”»å‡»ï¼š$\ell_0, \ell_2, \ell_\infty$
      2. ä¸å»æ›´æ–°$\delta$ï¼Œè€Œæ˜¯æ›´æ–°$w_i$
      3. $\ell_2$æ”»å‡»ï¼š
         1. $\delta_i = \frac{1}{2}(\tanh(w_i)+1)-x_i \in [0,1]$
            1. $-1 \leq \tanh(w_i) \leq 1$
         2. é—®é¢˜å®šä¹‰ï¼š$\mathrm{minimize}(\Vert \frac{1}{2}(\tanh(w_i)+1)-x_i\Vert_p + c\cdot f(\frac{1}{2}(\tanh(w_i)+1))$

é€šè¿‡ä»¥ä¸Šé—®é¢˜å®šä¹‰ï¼Œä½¿ç”¨æ¢¯åº¦ä¸‹é™ç­‰æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œå¾—åˆ°åˆé€‚çš„$w_i$ï¼Œå³å¯ç”Ÿæˆå¯¹æŠ—æ ·æœ¬$\frac{1}{2}(\tanh(w_i)+1)+1$

### DeepFool

#### è®ºæ–‡ç¬”è®°

[DeepFool: a simple and accurate method to fool deep neural networks](https://arxiv.org/pdf/1511.04599)

##### é—®é¢˜å®šä¹‰

å¯¹äºç»™å®šçš„åˆ†ç±»å™¨ï¼Œå°†å¯¹æŠ—æ€§æ‰°åŠ¨å®šä¹‰ä¸ºæœ€å°æ‰°åŠ¨$r$ï¼Œè¯¥æ‰°åŠ¨è¶³ä»¥æ”¹å˜ä¼°è®¡çš„æ ‡ç­¾$\hat{k}(x)$ï¼š

$\begin{equation}
    \Delta(x;\hat{k}) := min_r \|r\|_2\ subject\ to\ \hat{k}(x+r) \neq \hat{k}(x)
\end{equation}$ï¼Œå…¶ä¸­$x$æ˜¯å›¾åƒï¼Œ$\hat{k}(x)$æ˜¯ä¼°è®¡çš„æ ‡ç­¾ã€‚

å®šä¹‰$\Delta(x;\hat{k})$ä¸ºåœ¨$x$ä¸Š$\hat{k}$çš„é²æ£’æ€§ï¼Œå³æ‰¾åˆ°ä¸€ä¸ªå‘é‡$r$ï¼Œä½¿å¾—$\Vert r\Vert_2$æœ€å°ï¼Œå¹¶ä¸”$\hat{k}(x+r) \neq \hat{k}(x)$ã€‚

åˆ†ç±»å™¨$\hat{k}$çš„é²æ£’æ€§å®šä¹‰ä¸ºï¼š$\rho_{adv}(\hat{k}) = E_x \frac{\Delta(x;\hat{k})}{\|x\|_2}$ï¼Œå¯¹äºç»™å®šçš„åˆ†ç±»å™¨$\hat{k}$ï¼Œè®¡ç®—æ¯ä¸ªè¾“å…¥$x$æ‰€éœ€çš„æœ€å°æ‰°åŠ¨å¤§å°$\Delta(x;\hat{k})$ä»¥æ”¹å˜å…¶åˆ†ç±»ç»“æœï¼Œç„¶åå°†è¿™ä¸ªå€¼é™¤ä»¥$x$çš„$L_2$èŒƒæ•°ä»¥å½’ä¸€åŒ–ï¼Œæœ€åå¯¹æ‰€æœ‰è¿™æ ·çš„æ¯”å€¼æ±‚å¹³å‡ã€‚

å¾—åˆ°çš„ç»“æœåæ˜ äº†åˆ†ç±»å™¨åœ¨é¢å¯¹å¯¹æŠ—æ€§æ”»å‡»æ—¶çš„æ•´ä½“é²æ£’æ€§æ°´å¹³â€”â€”å¹³å‡æ¥è¯´ï¼Œéœ€è¦å¤šå¤§çš„ç›¸å¯¹æ‰°åŠ¨æ‰èƒ½è®©åˆ†ç±»å™¨å‡ºé”™ã€‚

è¾ƒé«˜çš„$\rho_{adv}(\hat{k})$å€¼æ„å‘³ç€åˆ†ç±»å™¨æ›´é²æ£’ï¼Œå› ä¸ºéœ€è¦æ›´å¤§çš„ç›¸å¯¹æ‰°åŠ¨æ‰èƒ½ä½¿å…¶çŠ¯é”™ï¼›

ç›¸åï¼Œè¾ƒä½çš„å€¼è¡¨æ˜åˆ†ç±»å™¨æ›´å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ã€‚

##### äºŒåˆ†ç±»å™¨ä¸­çš„DeepFool

å¤šç±»åˆ†ç±»å™¨å¯ä»¥è§†ä¸ºäºŒåˆ†ç±»å™¨çš„é›†åˆã€‚

è¿™é‡Œå‡è®¾$\hat{k}(x) = \mathrm{sign}(f(x))$ï¼Œå…¶ä¸­$f$æ˜¯ä»»æ„æ ‡é‡å€¼å›¾åƒåˆ†ç±»å‡½æ•°$f : R^n \to R$ï¼Œå®šä¹‰åˆ†ç±»è¾¹ç•Œ*ä¸º $f(x) = 0$ã€‚*

æˆ‘ä»¬é¦–å…ˆåˆ†æä»¿å°„åˆ†ç±»å™¨ $f(x) = w^Tx + b$ çš„æƒ…å†µï¼Œç„¶åæ¨å¹¿é€šç”¨ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ç”¨äºä»»ä½•å¯å¾®åˆ†çš„äºŒåˆ†ç±»å™¨ $f$ã€‚

åœ¨åˆ†ç±»å™¨$f$æ˜¯ä»¿å°„çš„æƒ…å†µä¸‹ï¼Œå¾ˆå®¹æ˜“çœ‹å‡º $f$ åœ¨ç‚¹ $x_0$ çš„é²æ£’æ€§ $\Delta(x_0; f)$ ç­‰äºä» $x_0$ åˆ°åˆ†ç¦»ä»¿å°„å¹³é¢çš„è·ç¦» $F = \{x : w^Tx + b = 0\}$ã€‚

æ”¹å˜åˆ†ç±»å™¨å†³ç­–çš„æœ€å°æ‰°åŠ¨å¯¹åº”äº $x_0$ åœ¨ F ä¸Šçš„æ­£äº¤æŠ•å½±ã€‚

![](å¯¹æŠ—æ ·æœ¬/image-20241127150119988.png)

å¯å¾—å‡º $r_*(x_0) := \arg \min_{r} \|r\|_2\ \text{subject to } \mathrm{sign}(f(x_0 + r)) \neq \mathrm{sign}(f(x_0))$

ç”±äº$f(x)=w^Tx+b$ï¼Œè¯¥å¼å­å­˜åœ¨è§£æè§£ï¼š$r_*(x_0) := -\frac{f(x_0)}{\|w\|^2} w$

ç°åœ¨å‡è®¾$f$æ˜¯ä¸€ä¸ªé€šç”¨çš„äºŒå¯å¾®åˆ†ç±»å™¨ï¼Œé‡‡ç”¨ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹æ¥ä¼°è®¡é²æ£’æ€§$\Delta(x_0;f)$ã€‚

å…·ä½“æ¥è¯´ï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œ$f$å›´ç»•å½“å‰ç‚¹$x_i$â€‹çº¿æ€§åŒ–ï¼Œçº¿æ€§åŒ–åˆ†ç±»å™¨çš„æœ€å°æ‰°åŠ¨è®¡ç®—ä¸º$\arg \min_{r_i} \|r_i\|_2 \quad \text{subject to } f(x_i) + \nabla f(x_i)^Tr_i = 0$

> $\mathrm{sign}(f(x_0 + r)) \neq \mathrm{sign}(f(x_0))$ï¼Œç”¨æ³°å‹’å±•å¼€æ¥è¿‘ä¼¼$f$åœ¨$x_i$é™„è¿‘çš„å€¼$f(x_i+r_i) \approx f(x_i) + \nabla f(x_i)^Tr_i$ï¼Œå¯ä»¥è½¬åŒ–ä¸º$f(x_i) + \nabla f(x_i)^Tr_i = 0$

ç®—æ³•ä¼ªä»£ç ï¼š

> **Algorithm 1** DeepFool for Binary Classifiers 
>
> 1. **Input:** Image $ \mathbf{x} $, classifier $ f $. 
> 2. **Output:** Perturbation $ \hat{\mathbf{r}} $. 
> 3. Initialization:  $\mathbf{x}_0 \leftarrow \mathbf{x}$,  $i \leftarrow 0$. 
> 4. **While** $ \text{sign}(f(\mathbf{x}_i)) = \text{sign}(f(\mathbf{x}_0)) $ **do**
>    1. $\mathbf{r}_i \leftarrow -\frac{f(\mathbf{x}_i)}{\lVert \nabla f(\mathbf{x}_i) \rVert_2^2} \nabla f(\mathbf{x}_i)$
>    2. $\mathbf{x}_{i+1} \leftarrow \mathbf{x}_i + \mathbf{r}_i$
>    3.  $i \leftarrow i + 1$
> 5. **end while**
> 6. **return** $\hat{\mathbf{r}} = \sum_i \mathbf{r}_i.$

##### å¤šç±»åˆ†ç±»å™¨ä¸­çš„DeepFool

åˆ†ç±»å™¨å…·æœ‰$c$ä¸ªè¾“å‡ºï¼Œå…¶ä¸­$c$æ˜¯ç±»çš„æ•°é‡ã€‚

å› æ­¤ï¼Œå¯ä»¥å°†åˆ†ç±»å™¨å®šä¹‰ä¸ºï¼š$f:R_n \to R_c$ï¼Œå¹¶é€šè¿‡ä»¥ä¸‹æ˜ å°„å®Œæˆåˆ†ç±»ï¼š $\hat{k}(\boldsymbol{x}) = \underset{k}{\text{arg max}} ~ f_k(\boldsymbol{x})$ï¼Œ$f_k(x)$æ˜¯ä¸ç¬¬$k$ä¸ªç±»ç›¸å¯¹åº”çš„$f(x)$çš„è¾“å‡ºã€‚

###### ä»¿å°„åˆ†ç±»å™¨

å‡è®¾$f(x)$ä¸ºä»¿å°„åˆ†ç±»å™¨$f(x) = W^Tx+b$ï¼Œåˆ™æœ€å°æ‰°åŠ¨ä¸ºï¼š
$$
\underset{\boldsymbol{r}}{\operatorname{arg}\,\operatorname{min}} \left \| \boldsymbol{r} \right \|_2 \\
s.t.\; \exists k: \boldsymbol{w}_{k}^\top(\boldsymbol{x}_0+\boldsymbol{r}) + b_k \geq \boldsymbol{w}_{\hat{k}(\boldsymbol{x}_0)}^\top(\boldsymbol{x}_0+\boldsymbol{r}) + b_{\hat{k}(\boldsymbol{x}_0)}
$$
å…¶ä¸­ï¼Œ$\boldsymbol{w}_k$ æ˜¯$W$çš„ç¬¬$k$åˆ—ã€‚

åœ¨å‡ ä½•ä¸Šï¼Œä¸Šè¿°é—®é¢˜å¯¹åº”äº $\boldsymbol{x}_0$ ä¸å‡¸å¤šé¢ä½“Pçš„**è¡¥æ•°**ä¹‹é—´çš„è·ç¦»çš„è®¡ç®—ï¼š$P = \bigcap_{k=1}^{c} \{ \boldsymbol{x}: f_{\hat{k}(\boldsymbol{x}_0)}(\boldsymbol{x}) \geq f_k(\boldsymbol{x}) \}$ï¼Œå³$P$æ˜¯æ‰€æœ‰æ»¡è¶³æ¡ä»¶ $f_k(x) \geq f(x)$çš„ç‚¹$x$æ„æˆçš„é›†åˆçš„äº¤é›†ï¼Œå…¶ä¸­$k$éå†æ‰€æœ‰ç±»åˆ«$1$åˆ°$c$ã€‚

å°†$\hat{l}(x_0)$å®šä¹‰ä¸º$P$è¾¹ç•Œçš„æœ€è¿‘è¶…å¹³é¢ï¼š$\hat{l}(x_0) = \underset{k \neq \hat{k}(x_0)} {\arg \min} \frac{|f_k(x_0) - f_{\hat{k}(x_0)}(x_0)|}{||w_k - w_{\hat{k}(x_0)}||_2}$ï¼Œ

å³æ±‚ç¦»$x_0$æœ€è¿‘çš„è¶…å¹³é¢ï¼Œå¯ä»¥ä½¿$f_{\hat{k}(\boldsymbol{x}_0)}(\boldsymbol{x}) \leq f_k(\boldsymbol{x})\ s.t. k \neq \hat{k}(x_0)$

ä¸€æ—¦ç¡®å®šäº†æœ€è¿‘çš„è¶…å¹³é¢$\hat{l}(x_0)$ï¼Œå°±å¯ä»¥è®¡ç®—æœ€å°æ‰°åŠ¨$r_*(x_0)$ã€‚

è¿™ä¸ªæ‰°åŠ¨æ˜¯å°†$x_0$æŠ•å½±åˆ°æœ€è¿‘çš„è¶…å¹³é¢ä¸Šçš„å‘é‡ï¼Œå…·ä½“å…¬å¼å¦‚ä¸‹ï¼š
$$
r_*(x_0) = \frac{|f_{\hat{l}(x_0)}(x_0) - f_{\hat{k}(x_0)}(x_0)|}{||w_{\hat{l}(x_0)} - w_{\hat{k}(x_0)}||_2} (w_{\hat{l}(x_0)} - w_{\hat{k}(x_0)})
$$

###### é€šç”¨åˆ†ç±»å™¨

å¯¹äºä¸€èˆ¬çš„éçº¿æ€§åˆ†ç±»å™¨ï¼Œæè¿°åˆ†ç±»å™¨è¾“å‡ºæ ‡ç­¾$\hat{k}(x_0)$çš„ç©ºé—´åŒºåŸŸçš„é›†åˆ$P$ä¸å†æ˜¯å¤šé¢ä½“ã€‚

åœ¨äºŒåˆ†ç±»æƒ…å†µä¸‹éµå¾ªè§£é‡Šçš„è¿­ä»£çº¿æ€§åŒ–è¿‡ç¨‹ï¼Œæˆ‘ä»¬åœ¨è¿­ä»£$i$å¤„é€šè¿‡å¤šé¢ä½“$\widetilde{P}_i$è¿‘ä¼¼é›†åˆ$P$
$$
\widetilde{P}_i = \bigcap_{k=1}^{c} \left\{ \mathbf{x} : f_k(\mathbf{x}_i) - f_{\hat{k}(\mathbf{x}_0)}(\mathbf{x}_i) + \nabla f_k(\mathbf{x}_i)^{\top} \mathbf{x} - \nabla f_{\hat{k}(\mathbf{x}_0)}(\mathbf{x}_i)^{\top} \mathbf{x} \leq 0 \right\}
$$

$f_k(\mathbf{x}_i) - f_{\hat{k}(\mathbf{x}_0)}(\mathbf{x}_i)$è¡¡é‡çš„æ˜¯åœ¨ç‚¹$x_i$ä¸Šï¼Œåˆ†ç±»å™¨å¯¹ç±»åˆ«$k$çš„è¾“å‡ºä¸å¯¹åŸå§‹è¾“å…¥$x_0$é¢„æµ‹ç±»åˆ«çš„è¾“å‡ºä¹‹å·®ã€‚

$\nabla f_k(\mathbf{x}_i)^{\top} \mathbf{x} - \nabla f_{\hat{k}(\mathbf{x}_0)}(\mathbf{x}_i)^{\top} \mathbf{x}$æ˜¯å…³äºæ–°ç‚¹$x$çš„çº¿æ€§é¡¹ï¼Œåˆ©ç”¨äº†åˆ†ç±»å™¨åœ¨ç‚¹$x_i$ä¸Šå¯¹ç±»åˆ«$k$å’ŒåŸå§‹é¢„æµ‹ç±»åˆ«çš„æ¢¯åº¦ï¼Œæä¾›äº†å±€éƒ¨å˜åŒ–ç‡çš„ä¿¡æ¯ï¼Œå› æ­¤è¿™ä¸ªçº¿æ€§é¡¹æè¿°äº†å½“åœ¨$x_i$å‘¨å›´ç§»åŠ¨æ—¶ï¼Œåˆ†ç±»å™¨å¯¹è¿™ä¸¤ä¸ªç±»åˆ«è¾“å‡ºçš„å˜åŒ–æƒ…å†µã€‚

> **Algorithm 2** DeepFool for Multi-Class Classifiers
>
> 1. **Input:** Image $ \mathbf{x} $, classifier $ f $.
> 2. **Output:** Perturbation $ \hat{\mathbf{r}} $.
> 3. Initialization: $\mathbf{x}_0 \leftarrow \mathbf{x}$, $i \leftarrow 0$.
> 4. **While** $ \hat{k}(\mathbf{x}_i) = \hat{k}(\mathbf{x}_0) $ **do**
>    1. **For** $k \neq \hat{k}(\mathbf{x}_0)$ **do** 
>       1. $\mathbf{w}'_k \leftarrow \nabla f_k(\mathbf{x}_i) - \nabla f_{\hat{k}(\mathbf{x}_0)}(\mathbf{x}_i)$
>       2. $f'_k \leftarrow f_k(\mathbf{x}_i) - f_{\hat{k}(\mathbf{x}_0)}(\mathbf{x}_i)$
>    2. **End For**
>    3. $\hat{l} \leftarrow \arg\min_{k \neq \hat{k}(\mathbf{x}_0)} \frac{|f'_{k}|}{\|\mathbf{w}'_{k}\|_2}$ ï¼ˆé€‰æ‹©æœ€æœ‰å¯èƒ½å°†å½“å‰é¢„æµ‹ç±»åˆ«å˜ä¸ºå…¶ä»–ç±»åˆ«çš„æ‰°åŠ¨æ–¹å‘ï¼‰
>    4. $\mathbf{r}_i \leftarrow \frac{|f'_{\hat{l}}|}{\|\mathbf{w}'_{\hat{l}}\|^2_2} \mathbf{w}'_{\hat{l}}$
>    5. $\mathbf{x}_{i+1} \leftarrow \mathbf{x}_i + \mathbf{r}_i$
>    6. $i \leftarrow i + 1$
> 5. **end while**
> 6. **return** $\hat{\mathbf{r}} = \sum_i \mathbf{r}_i$

1. $f'_{k}$ä»£è¡¨ç¬¬$k$ç±»ç›¸å¯¹äºå½“å‰é¢„æµ‹ç±»åˆ«çš„åˆ†æ•°å·®ã€‚
2. $\mathbf{w}'_{k}$æ˜¯ä¸€ä¸ªæ¢¯åº¦å·®å¼‚å‘é‡ï¼Œè¡¨ç¤ºä»å½“å‰é¢„æµ‹ç±»åˆ«åˆ°ç¬¬$k$ç±»çš„å†³ç­–è¾¹ç•Œçš„æ–¹å‘ã€‚
3. $\frac{|f'_{k}|}{\|\mathbf{w}'_{k}\|_2}$è¡¡é‡äº†è¦ä½¿å½“å‰é¢„æµ‹ç±»åˆ«å˜ä¸ºç¬¬$k$ç±»æ‰€éœ€çš„è·ç¦»ã€‚
   1. ç»å¯¹å€¼$|f'_{k}|$ç¡®ä¿æ— è®º$f'_{k}$æ˜¯æ­£è¿˜æ˜¯è´Ÿï¼Œéƒ½åªè€ƒè™‘è·ç¦»çš„å¤§å°ã€‚
   2. é™¤ä»¥${\|\mathbf{w}'_{\hat{l}}\|^2_2}$åˆ™æ˜¯å¯¹æ–¹å‘è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä½¿å…¶ä¸å—æ¢¯åº¦é•¿åº¦çš„å½±å“ã€‚



#### ä»£ç å®ç°

[å¯¹æŠ—æ ·æœ¬ä¹‹DeepFoolåŸç†&coding](https://blog.csdn.net/weixin_41466575/article/details/118978636?spm=1001.2014.3001.5502)ğŸ‘ˆåœ¨å…¶ä»£ç ä¸Šè¿›è¡Œæ³¨é‡Š

```python
import numpy as np
from torch.autograd import Variable
import torch as torch
import copy

def deepfool(image, net, num_classes=10, overshoot=0.02, max_iter=100):
    is_cuda = torch.cuda.is_available()
    if is_cuda:
        image = image.cuda()
        net = net.cuda
    # è·å–åŸå§‹è¾“å…¥çš„åˆ†ç±»ä¿¡æ¯
    f_image = net.forward(Variable(image[None, :, :, :], requires_grad=True)).data.cpu().numpy().flatten()
    # flattenå±•ä¸ºä¸€ç»´çš„ï¼Œargsortä»å°åˆ°å¤§åˆ†ç±»çš„ç´¢å¼•ï¼Œ[::-1]æ”¹ä¸ºä»å¤§åˆ°å°åˆ†ç±»çš„ç´¢å¼•
    I = (np.array(f_image)).flatten().argsort()[::-1]
    # Iä¸ºæ¨¡å‹è¾“å‡ºçš„æ‰€æœ‰ç±»åˆ«çš„å¯ä¿¡åº¦çš„ç´¢å¼•ï¼ˆä»å¤§åˆ°å°ï¼‰
    I = I[0:num_classes]
    # labelä¸ºæ¨¡å‹æ­£å¸¸é¢„æµ‹çš„ç±»åˆ«
    label = I[0]
    
    input_shape = image.cpu().numpy().shape
    pert_image = copy.deepcopy(image)
    w = np.zeros(input_shape)
    r_tot = np.zeros(input_shape)

    loop_i = 0

    x = Variable(pert_image[None, :], requires_grad=True)
    fs = net.forward(x)
    # æŒ‰ç…§ç´¢å¼•å°†fsè¿›è¡Œæ’åˆ—
    fs_list = [fs[0,I[k]] for k in range(num_classes)]
    k_i = label

    while k_i == label and loop_i < max_iter:
        pert = np.inf
        # è·å–f_{k(x0)}(x)çš„æ¢¯åº¦
        fs[0, I[0]].backward(retain_graph=True)
        grad_orig = x.grad.data.cpu().numpy().copy()
		# éå†é™¤æ­£å¸¸é¢„æµ‹æ‰€æœ‰ç±»åˆ«
        for k in range(1, num_classes):
            if x.grad is not None:
            	x.grad.zero_()
            # è·å–å½“å‰éå†åˆ°çš„ç±»åˆ«çš„f_{k}(x)çš„æ¢¯åº¦
            fs[0, I[k]].backward(retain_graph=True)
            cur_grad = x.grad.data.cpu().numpy().copy()
            # æ›´æ–°w_kå’Œf_k
            w_k = cur_grad - grad_orig
            f_k = (fs[0, I[k]] - fs[0, I[0]]).data.cpu().numpy()
			# è®¡ç®—è¯¥ç±»åˆ«çš„
            pert_k = abs(f_k)/np.linalg.norm(w_k.flatten())
            # ç›¸å½“äºè¿›è¡Œhat l = arg min(...)æ“ä½œ
            if pert_k < pert:
                pert = pert_k
                w = w_k
        # compute r_i and r_tot
        # Added 1e-4 for numerical stability
        r_i =  (pert+1e-4) * w / np.linalg.norm(w)
        x_i = np.float32(r_tot + r_i)

        # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
        if is_cuda:
            pert_image = image + (1+overshoot)*torch.from_numpy(x_i).cuda()
        else:
            pert_image = image + (1+overshoot)*torch.from_numpy(x_i)
		# è·å–æ–°çš„åˆ†ç±»é¢„æµ‹
        x = Variable(pert_image, requires_grad=True)
        fs = net.forward(x)
        k_i = np.argmax(fs.data.cpu().numpy().flatten())

        loop_i += 1
    
    x_i = (1+overshoot) * x_i
    return x_i, loop_i, label, k_i, pert_image


# è¿™å‡ ä¸ªå˜é‡ä¸»è¦ç”¨äºä¹‹åçš„æµ‹è¯•ä»¥åŠå¯è§†åŒ–
adver_example_by_FOOL = torch.zeros((batch_size,1,28,28)).to(device)
adver_target = torch.zeros(batch_size).to(device)
clean_example = torch.zeros((batch_size,1,28,28)).to(device)
clean_target = torch.zeros(batch_size).to(device)
# ä»test_loaderä¸­é€‰å–1000ä¸ªå¹²å‡€æ ·æœ¬ï¼Œä½¿ç”¨deepfoolæ¥ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
for i,(data,target) in enumerate(test_loader):
	if i >= adver_nums/batch_size :
		break
	if i == 0:
    	clean_example = data
    else:
        clean_example = torch.cat((clean_example,data),dim = 0)
    cur_adver_example_by_FOOL = torch.zeros_like(data).to(device)
    for j in range(batch_size):
        r_rot,loop_i,label,k_i,pert_image = deepfool(data[j],simple_model)
        cur_adver_example_by_FOOL[j] = pert_image
  	# ä½¿ç”¨å¯¹æŠ—æ ·æœ¬æ”»å‡»VGGæ¨¡å‹
    pred = simple_model(cur_adver_example_by_FOOL).max(1)[1]
	# print (simple_model(cur_adver_example_by_FOOL).max(1)[1])
    if i == 0:
        adver_example_by_FOOL = cur_adver_example_by_FOOL
        clean_target = target
        adver_target = pred
    else:
        adver_example_by_FOOL = torch.cat((adver_example_by_FOOL , cur_adver_example_by_FOOL), dim = 0)
        clean_target = torch.cat((clean_target,target),dim = 0)
        adver_target = torch.cat((adver_target,pred),dim = 0)

print (adver_example_by_FOOL.shape)
print (adver_target.shape)
print (clean_example.shape)
print (clean_target.shape)
```

### Universal Adversarial Perturbations

Universal Adversarial Perturbationsèƒ½ç”Ÿæˆ**å¯¹ä»»ä½•å›¾åƒå®ç°æ”»å‡»**çš„æ‰°åŠ¨ã€‚

#### è®ºæ–‡ç¬”è®°

[Universal Adversarial Perturbations](https://ieeexplore.ieee.org/document/8099500)

ä¸ºäº†æ±‚è§£ä¸€ä¸ªé€šç”¨æ‰°åŠ¨ $v$ ï¼Œä½¿å¾—ï¼š
$$
\hat{k}(x + v) \neq \hat{k}(x) \text{ for "most" } x \sim \mu
$$
$\mu$ ä»£è¡¨å›¾åƒçš„æ•°æ®åˆ†å¸ƒï¼Œ $\hat{k}(x)$ æ˜¯å¯¹åº”è¾“å…¥ $x$ ç½‘ç»œè¾“å‡ºçš„é¢„æµ‹ç»“æœï¼Œé™„åŠ çš„çº¦æŸæ¡ä»¶ä¸ºï¼š
$$
\|v\|_p \leq \xi \\
\mathbb{P}_{x \sim \mu} \left( \hat{k}(x + v) \neq \hat{k}(x) \right) \geq 1 - \delta
$$
$\xi$ ä»£è¡¨äº†æ‰°åŠ¨å‘é‡çš„é‡çº§ï¼Œ $\delta$ å®šé‡äº†å¸Œæœ›æ¬ºéª—çš„æ¯”ä¾‹ã€‚

åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œè®¡ç®—å°†å½“å‰æ‰°åŠ¨ç‚¹$x_i + v$å‘é€åˆ°åˆ†ç±»å™¨å†³ç­–è¾¹ç•Œçš„æœ€å°æ‰°åŠ¨$\Delta v_i$ï¼Œå¹¶å°†å…¶æ±‡æ€»åˆ°é€šç”¨æ‰°åŠ¨çš„å½“å‰å®ä¾‹$v$ã€‚

å¦‚æœå½“å‰çš„é€šç”¨æ‰°åŠ¨ $ v $ ä¸èƒ½æ¬ºéª—æ•°æ®ç‚¹ $ x_i $ï¼Œå¯ä»¥é€šè¿‡è§£å†³ä»¥ä¸‹ä¼˜åŒ–é—®é¢˜æ¥å¯»æ±‚å…·æœ‰æœ€å°èŒƒæ•°çš„é¢å¤–æ‰°åŠ¨ $ \Delta v_i $ æ¥æ¬ºéª—æ•°æ®ç‚¹ $ x_i $ï¼š
$$
\Delta v_i \gets \arg \min_r \|r\|_2 \text{ s.t. } \hat{k}(x_i + v + r) \neq \hat{k}(x_i).
$$
åŒæ—¶ä¸ºäº†ä¿è¯ $ \|v\|_p < \xi $ï¼Œæ›´æ–°åçš„æ‰°åŠ¨ä¼šè¢«æŠ•å½±ï¼Œå¦‚ä¸‹ï¼š

$$
\mathcal{P}_{p,\xi}(v) = \arg \min_{v'} \|v - v'\|_2 \text{ subject to } \|v'\|_p \leq \xi.
$$
$v' = v + \Delta v_i$æ˜¯æŒ‡åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­è°ƒæ•´åçš„æ‰°åŠ¨å‘é‡ã€‚

è¿™ä¸ªæŠ•å½±æ“ä½œå¯»æ‰¾æœ€æ¥è¿‘åŸå§‹æ‰°åŠ¨$v$çš„æ–°å‘é‡$v'$ï¼ŒåŒæ—¶ä¿è¯$v'$çš„$L_p$èŒƒæ•°ä¸è¶…è¿‡$\xi$ã€‚

- å¦‚æœ$v$çš„$L_p$èŒƒæ•°å·²ç»å°äºæˆ–ç­‰äº$\xi$ï¼Œåˆ™æŠ•å½±æ“ä½œä¸ä¼šæ”¹å˜$v$ã€‚
- å¦‚æœ$v$çš„$L_p$èŒƒæ•°å¤§äº$\xi$ï¼Œåˆ™éœ€è¦æ‰¾åˆ°ä¸€ä¸ªæœ€æ¥è¿‘$v$ä¸”å…¶$L_p$èŒƒæ•°ä¸è¶…è¿‡$\xi$çš„å‘é‡$v'$ã€‚

ä¼ªä»£ç ï¼š

> **Algorithm 1 Computation of Universal Perturbations** 
>
> 1. **Input:** Data points $X$, classifier $\hat{k}$, desired $\ell_p$ norm of the perturbation $\xi$, desired accuracy on perturbed samples $\delta$. 
> 2. **Output:** Universal perturbation vector $v$. 
> 3. Initialize $v \leftarrow 0$. 
> 4. **while** $\operatorname{Err}(X_v) > 1-\delta$ **do**
>    1. **For** each data point $x_i \in X$ **do** 
>       1. **If** $\hat{k}(x_i + v) = \hat{k}(x_i)$ **then**
>          1. Compute the minimal perturbation that sends $x_i + v$ to the decision boundary:$\Delta v_i \leftarrow \arg \min_{r} \|r\|_2 \text{ s.t. } \hat{k}(x_i + v + r) \neq \hat{k}(x_i).$
>          2. Update the perturbation: $v \leftarrow \mathcal{P}_{p,\xi}(v + \Delta v_i)$
>       2. **End if** 
>    2. **End for**
> 5. **end while**

#### ä»£ç å®ç°

æˆªè‡ªğŸ‘‰[LTS4/universal](https://github.com/LTS4/universal)

+ å¯»æ‰¾æœ€å°æ‰°åŠ¨ä½¿ç”¨çš„DeepFoolæ–¹æ³•

```python
import numpy as np
from deepfool import deepfool

def proj_lp(v, xi, p):
    if p == 2:
        v = v * min(1, xi/np.linalg.norm(v.flatten(1)))
        # v = v / np.linalg.norm(v.flatten(1)) * xi
    elif p == np.inf:
        v = np.sign(v) * np.minimum(abs(v), xi)
    else:
         raise ValueError('Values of p different from 2 and Inf are currently not supported...')
    return v

def universal_perturbation(dataset, f, grads, delta=0.2, max_iter_uni = np.inf, xi=10, p=np.inf, num_classes=10, overshoot=0.02, max_iter_df=10):
    ...
    while fooling_rate < 1-delta and itr < max_iter_uni:
        # Shuffle the dataset
        np.random.shuffle(dataset)

        print ('Starting pass number ', itr)

        # Go through the data set and compute the perturbation increments sequentially
        for k in range(0, num_images):
            cur_img = dataset[k:(k+1), :, :, :]

            if int(np.argmax(np.array(f(cur_img)).flatten())) == int(np.argmax(np.array(f(cur_img+v)).flatten())):
                print('>> k = ', k, ', pass #', itr)

                # Compute adversarial perturbation
                dr,iter,_,_ = deepfool(cur_img + v, f, grads, num_classes=num_classes, overshoot=overshoot, max_iter=max_iter_df)

                # Make sure it converged...
                if iter < max_iter_df-1:
                    v = v + dr

                    # Project on l_p ball
                    v = proj_lp(v, xi, p)

    ...
```

### UPSETã€ANGRI

[1707.01159](https://arxiv.org/pdf/1707.01159)

UPSETçš„$R$å’ŒANGRIçš„$A_t, A_x, A_c$çš„å…·ä½“å®ç°å¯ä»¥çœ‹è®ºæ–‡ï¼Œå¯¹äºä¸åŒçš„æ•°æ®é›†ï¼Œæœ‰äº›è®¸ä¸ä¸€æ ·ã€‚

#### UPSET: Universal Perturbations for Steering to Exact Targets

åœ¨ä¸€ä¸ªåŒ…å« n ä¸ªç±»åˆ«çš„è®¾ç½®ä¸­ï¼ŒUPSET çš„ç›®æ ‡æ˜¯ç”Ÿæˆ n ä¸ª**é€šç”¨æ‰°åŠ¨**$r_j\ (j \in \left\{1, 2, ...., n\right\}$)ï¼Œå½“$r_j$è¢«æ·»åŠ åˆ°ä¸å±äºç±»åˆ« j çš„ä»»ä½•å›¾åƒæ—¶ï¼Œåˆ†ç±»å™¨ä¼šå°†ç»“æœå›¾åƒåˆ¤å®šä¸ºå±äºç±»åˆ« jã€‚

UPSETåˆ©ç”¨ä¸€ä¸ªæ®‹å·®ç”Ÿæˆç½‘ç»œ$R$ï¼Œå®ƒæ¥æ”¶ç›®æ ‡ç±»åˆ«$t$ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªä¸è¾“å…¥å›¾åƒ$X$ç»´åº¦ç›¸åŒçš„æ‰°åŠ¨$r_t = R(t)$ã€‚

å¯¹æŠ—å›¾åƒé€šè¿‡ä»¥ä¸‹å…¬å¼ç”Ÿæˆï¼š$\hat{x} = U(x, t)=\max(\min(s \times R(t) + x, 1), -1)$

+ ç¼©æ”¾å› å­$s$è®¾ç½®ä¸º$2$æ—¶ï¼Œç¡®ä¿äº†$s \times R(t)$å¯ä»¥å°†è¾“å…¥ç©ºé—´$[-1, 1]$ä¸­çš„ä»»ä½•å€¼è½¬æ¢ä¸º$[-1, 1]$å†…çš„ä»»ä½•å€¼ã€‚
  + ä½†å¯ä»¥é€‰æ‹©è¾ƒä½çš„$s$å€¼ï¼Œé™åˆ¶æ¯ä¸ªåƒç´ çš„æœ€å¤§å¯èƒ½æ®‹å·®å€¼
+ åœ¨æ·»åŠ æ‰°åŠ¨åï¼Œè¾“å‡ºå€¼è¢«è£å‰ªå›$[-1, 1]$åŒºé—´ï¼Œä»¥ä¿è¯ç”Ÿæˆæœ‰æ•ˆçš„å›¾åƒã€‚

æµç¨‹å¦‚ä¸‹ï¼š

![](å¯¹æŠ—æ ·æœ¬/image-20241128143952614.png)

#### ANGRI: Antagonistic Network for Generating Rogue Images for targeted fooling of deep neural networks

ANGRI çš„ç›®æ ‡æ˜¯ç»™å®šä¸€ä¸ªè¾“å…¥å›¾åƒ$x$å’Œä¸€ä¸ªç›®æ ‡ç±»åˆ«$t$ï¼Œå°†å›¾åƒè½¬æ¢ä¸ºä¸€ä¸ªæ–°çš„å›¾åƒ$\hat{x}$ï¼Œä½¿å¾—åˆ†ç±»å™¨é”™è¯¯åœ°å°†å…¶è¯†åˆ«ä¸ºç›®æ ‡ç±»åˆ«$t$ã€‚

ANGRI é€šè¿‡ä¸€ä¸ªç”Ÿæˆç½‘ç»œ$A$æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œè¯¥ç½‘ç»œæ¥æ”¶åŸå§‹å›¾åƒå’Œç›®æ ‡ç±»åˆ«ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºå¯¹æŠ—å›¾åƒ$\hat{x} = A(x,t)$ã€‚

* ç”±ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ç»„æˆï¼š$A_t, A_x, A_c$
* $A_t$æ˜¯åŸºäºç›®æ ‡ç±»åˆ«çš„ç½‘ç»œï¼Œå®ƒæ¥æ”¶ç›®æ ‡ç±»åˆ«ä¿¡æ¯å¹¶ç”Ÿæˆä¸€ä¸ªä¸­é—´è¡¨ç¤ºã€‚
* $A_x$æ˜¯åŸºäºè¾“å…¥å›¾åƒçš„ç½‘ç»œï¼Œå®ƒæ¥æ”¶è¾“å…¥å›¾åƒå¹¶ç”Ÿæˆå¦ä¸€ä¸ªä¸­é—´è¡¨ç¤ºã€‚
* $A_c$æ˜¯ç»„åˆç½‘ç»œï¼Œå®ƒå°†$A_t,A_x$çš„è¾“å‡ºåˆå¹¶ï¼Œè¿›ä¸€æ­¥å¤„ç†ä»¥ç”Ÿæˆæœ€ç»ˆçš„å¯¹æŠ—å›¾åƒ$\hat{x}$ã€‚

æµç¨‹å¦‚ä¸‹ï¼š

![](å¯¹æŠ—æ ·æœ¬/image-20241128145123307.png)

#### æŸå¤±å‡½æ•°

UPSETå’ŒANGRIä½¿ç”¨çš„åŒä¸€ä¸ªæŸå¤±å‡½æ•°ã€‚

æŸå¤±å‡½æ•°$L$ç»“åˆäº†åˆ†ç±»æŸå¤±$L_C$å’Œä¿çœŸåº¦æŸå¤±$L_F$ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š 
$$
L(\mathbf{x}, \tilde{\mathbf{x}}, t) &=& L_C(\tilde{\mathbf{x}}, t) + L_F(\mathbf{x}, \tilde{\mathbf{x}}) \\
&=& -\sum_{i=1}^{m} log(C_i(\tilde{\mathbf{x}})[t]) + w \left\| \tilde{\mathbf{x}} - \mathbf{x} \right\|^k_k,
$$

- $L_C$æ˜¯åˆ†ç±»æŸå¤±ï¼Œå…·ä½“å®šä¹‰ä¸ºåˆ†ç±»å™¨å¯¹å¯¹æŠ—å›¾åƒ$\hat{x}$é¢„æµ‹ç›®æ ‡ç±»åˆ«$t$çš„è´Ÿå¯¹æ•°æ¦‚ç‡ã€‚
  - $C_i(\hat{x})$è¡¨ç¤ºç¬¬$i$ä¸ªé¢„è®­ç»ƒåˆ†ç±»å™¨å¯¹å¯¹æŠ—å›¾åƒ$\hat{x}$çš„è¾“å‡ºåˆ†ç±»æ¦‚ç‡åˆ†å¸ƒï¼Œè€Œ$C_i(\hat{x})[t]$åˆ™æ˜¯è¯¥åˆ†å¸ƒä¸­ç›®æ ‡ç±»åˆ«$t$çš„æ¦‚ç‡ã€‚
- $L_F$æ˜¯ä¿çœŸåº¦æŸå¤±ï¼Œç”¨æ¥è¡¡é‡ç”Ÿæˆçš„å¯¹æŠ—å›¾åƒ$\hat{x}$ä¸åŸå§‹å›¾åƒ$x$ä¹‹é—´çš„å·®å¼‚ï¼Œé€šå¸¸ä½¿ç”¨$L_2$èŒƒæ•°ï¼ˆå³$k=2$ï¼‰ã€‚
  - $w$æ˜¯ä¸€ä¸ªæƒé‡å‚æ•°ï¼Œç”¨äºå¹³è¡¡åˆ†ç±»æŸå¤±å’Œä¿çœŸåº¦æŸå¤±ä¹‹é—´çš„å…³ç³»ã€‚

### Houdini

[ Houdini: Fooling Deep Structured Prediction Models](https://arxiv.org/abs/1707.05373)

**æŸå¤±å‡½æ•°Houdini**æ˜¯ä¸€ç§çµæ´»çš„æ–¹æ³•ï¼Œæ—¨åœ¨ç›´æ¥æ ¹æ®ä»»åŠ¡æŸå¤±ç”Ÿæˆå¯¹æŠ—æ€§æ ·æœ¬ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¾èµ–äºåˆ†ç±»ä»»åŠ¡ä¸­å¸¸ç”¨çš„æ›¿ä»£æŸå¤±å‡½æ•°ã€‚å®ƒèƒ½å¤Ÿå¤„ç†é‚£äº›ä»»åŠ¡æŸå¤±ä¸ºç»„åˆä¸”ä¸å¯å¾®çš„é—®é¢˜ï¼Œæ¯”å¦‚è¯­éŸ³è¯†åˆ«ã€å§¿æ€ä¼°è®¡ä»¥åŠè¯­ä¹‰åˆ†å‰²ç­‰ã€‚

#### è®ºæ–‡ç¬”è®°

åœ¨ä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹$g_\theta$ä¸­ï¼Œä»»åŠ¡æŸå¤±å‡½æ•°ä¸º$\ell(\cdot)$ï¼Œå¹¶ä¸”å‡è®¾å¯¹äºä»»ä½•ç›®æ ‡$y$ï¼Œä»»åŠ¡æŸå¤±$\ell(y,y) = 0$ã€‚

å¯¹äºä¸€ä¸ªæ ·æœ¬$(x,y)$ï¼Œç½‘ç»œç»™å‡ºçš„å¾—åˆ†ä¸º$g_\theta(x, y)$ï¼Œå¹¶ä¸”ç½‘ç»œçš„è§£ç å™¨ä¼šé¢„æµ‹å‡ºå¾—åˆ†æœ€é«˜çš„ç›®æ ‡ï¼š$\hat{y} = y_{\theta}(x) = \underset{y \in Y}{\arg \max}\ g_{\theta}(x, y)$

å…³äºæ‰¾åˆ°ä¸€ä¸ªå¯¹æŠ—æ ·æœ¬$\tilde{x}$ï¼Œåœ¨é€‰å®šçš„ p èŒƒæ•°å’Œå™ªå£°å‚æ•°$\epsilon$çš„ä»»åŠ¡æŸå¤±$\ell(\cdot)$æ–¹é¢æ¬ºéª—æ¨¡å‹$g_\theta$çš„é—®é¢˜ï¼Œå¯å½’ç»“ä¸ºæ±‚è§£ï¼š
$$
\tilde{x} = \underset{\tilde{x} : \|\tilde{x} - x\|_p \leq \epsilon}{\arg \max} \ell(y_\theta(\tilde{x}), y)
$$
ä»¥ä¸Šæåˆ°çš„ä»»åŠ¡æŸå¤±$\ell(\cdot)$ç»å¸¸æ˜¯ç»„åˆæ€§çš„ï¼Œéš¾ä»¥ä¼˜åŒ–ï¼Œå› æ­¤é€šå¸¸ç”¨ä¸€ä¸ªå¯å¾®åˆ†çš„ä»£ç†æŸå¤±å‡½æ•°ä»£æ›¿ï¼Œè®°ä½œ$\hat{\ell}(y_\theta(\tilde{x}),y)$

æœ¬ç¯‡è®ºæ–‡æå‡ºçš„Houdiniçš„ä»£ç†æŸå¤±å‡½æ•°ï¼Œå…¶å®šä¹‰å¦‚ä¸‹ï¼š
$$
\bar{\ell}_H(\theta, x, y) = P_{\gamma \sim N(0,1)} \left[ g_\theta(x, y) - g_\theta(x, \hat{y}) < \gamma \right] \cdot \ell(\hat{y}, y)
$$
HoudiniæŸå¤±æ˜¯ç”±ä¸¤ä¸ªéƒ¨åˆ†ç»„æˆçš„ä¹˜ç§¯:

* éšæœºè¾¹é™…æ¦‚ç‡ï¼Œå³å®é™…ç›®æ ‡ $g_\theta(x, y)$ å’Œé¢„æµ‹ç›®æ ‡ $g_\theta(x, \hat{y})$ ä¹‹é—´çš„å¾—åˆ†å·®å°äºä»æ ‡å‡†æ­£æ€åˆ†å¸ƒé‡‡æ ·çš„å€¼ $\gamma$ çš„æ¦‚ç‡ã€‚
  * è¿™åæ˜ äº†æ¨¡å‹å¯¹å…¶é¢„æµ‹çš„ä¿¡å¿ƒã€‚
* ä»»åŠ¡æŸå¤± $\ell(\hat{y}, y)$ï¼Œå®ƒç‹¬ç«‹äºæ¨¡å‹ã€‚

å½“æ¨¡å‹å¯¹é¢„æµ‹ç›®æ ‡ $\hat{y}$ çš„ç½®ä¿¡åº¦æ— é™å¢åŠ æ—¶ï¼Œ$g_\theta(x, y) - g_\theta(x, \hat{y})$è¶Šæ¥è¶Šå°ï¼ŒHoudiniæŸå¤±æ”¶æ•›åˆ°ä»»åŠ¡æŸå¤±ã€‚

å…·ä½“æ¥è¯´ï¼šå½“$P_{\gamma \sim N(0,1)} (g_\theta(x, y) - g_\theta(x, \hat{y})\ \textless\ \gamma)$æ¥è¿‘äº1æ—¶ï¼Œ$\bar{\ell}_H(\theta, x, y) = \ell(\hat{y}, y)$ã€‚

å½“æ¨¡å‹å¯¹è‡ªå·±çš„é¢„æµ‹éå¸¸è‡ªä¿¡æ—¶ï¼ŒHoudiniæŸå¤±å°±èƒ½å‡†ç¡®åœ°åæ˜ ä»»åŠ¡æŸå¤±ï¼Œè¿™æ˜¯å› ä¸ºæ­¤æ—¶HoudiniæŸå¤±ä¸­çš„éšæœºè¾¹è·éƒ¨åˆ†ä¸å†èµ·ä½œç”¨ï¼Œæ•´ä¸ªæŸå¤±å‡½æ•°ç›´æ¥ç”±ä»»åŠ¡æŸå¤±å†³å®šã€‚

åœ¨å‰é¢çš„é—®é¢˜$\tilde{x} = \underset{\tilde{x} : \|\tilde{x} - x\|_p \leq \epsilon}{\arg \max} \ell(y_\theta(\tilde{x}), y)$ä¸­ï¼Œä½¿ç”¨HoudiniæŸå¤±å‡½æ•°æ›¿ä»£$\ell(\cdot)$ï¼Œå¹¶ä¸”ä½¿ç”¨ä¸€é˜¶è¿‘ä¼¼çš„æ–¹æ³•ã€‚

> è‹¥$\ell(\cdot)$æ˜¯å¯å¾®çš„ï¼Œåˆ™å¯ç”¨ä¸€ç§åŸºäºæ³°å‹’å±•å¼€çš„æ–¹æ³•æ¥è®¡ç®—æ‰°åŠ¨$\delta_x$
> $$
> \tilde{x} = \underset{\tilde{x} : \|\tilde{x} - x\|_p \leq \epsilon}{\arg \max} (\nabla_x\ell(g_\theta(x), y))^T\ (\tilde{x} - x)
> $$

è¯¥æ–¹æ³•éœ€è¦è®¡ç®—HoudiniæŸå¤±å‡½æ•°å…³äºè¾“å…¥$x$çš„æ¢¯åº¦ï¼Œé€šè¿‡é“¾å¼æ³•åˆ™ï¼Œå¯ä»¥å°†æ¢¯åº¦åˆ†è§£ä¸ºä¸¤éƒ¨åˆ†ï¼š
$$
\nabla_x \left[ \bar{\ell}_H(\theta, x, y) \right] = \frac{\partial \bar{\ell}_H(\theta, x, y)}{\partial g_\theta(x, y)} \frac{\partial g_\theta(x, y)}{\partial x}
$$
è¿™é‡Œçš„å…³é”®ç‚¹åœ¨äºï¼Œåªéœ€è¦è®¡ç®—HoudiniæŸå¤±å‡½æ•°å…³äºç½‘ç»œè¾“å‡º$g_\theta(x, y)$çš„å¯¼æ•°ï¼Œå…¶ä½™éƒ¨åˆ†å¯ä»¥é€šè¿‡åå‘ä¼ æ’­è·å¾—ã€‚

å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š

1. è®¡ç®—HoudiniæŸå¤±å‡½æ•°å…³äºç½‘ç»œè¾“å‡ºçš„å¯¼æ•°ï¼š$\nabla_g \left[ \mathbb{P}_{\gamma \sim \mathcal{N}(0,1)} \left[ g_\theta(x, y) - g_\theta(x, \hat{y}) < \gamma \right] \cdot \ell(y, \hat{y}) \right]$
2. å…·ä½“è®¡ç®—ï¼š$\nabla_g \left[ \mathbb{P}_{\gamma \sim \mathcal{N}(0,1)} \left[ g_\theta(x, y) - g_\theta(x, \hat{y}) < \gamma \right] \cdot \ell(y, \hat{y}) \right] = \nabla_g \left[ \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} e^{-v^2/2} dv\ \cdot\ \ell(y, \hat{y}) \right]$
3. ç®€åŒ–è¡¨è¾¾å¼ï¼š$\nabla_g \left[ \bar{\ell}_H(\hat{y}, y) \right] = \begin{cases} 
   -C \cdot e^{-|\delta g(y, \hat{y})|^2/2}\ \cdot\  \ell(y, \hat{y}), & g = g_\theta(x, y) \\
   C \cdot e^{-|\delta g(y, \hat{y})|^2/2}\ \cdot\  \ell(y, \hat{y}), & g = g_\theta(x, \hat{y}) \\
   0, & \text{otherwise}
   \end{cases}$
   1. $C = \frac{1}{\sqrt{2\pi}}$
   2. $\delta g(y, \hat{y}) = g_\theta(x, y) - g_\theta(x, \hat{y})$
   3. å½“$g = g_\theta(x,y)$ï¼Œå³ç½‘ç»œå¯¹çœŸå®æ ‡ç­¾$y$çš„è¾“å‡ºæ—¶ï¼Œæ¢¯åº¦ä¸ºè´Ÿã€‚
   4. å½“$g=g_\theta(x,\hat{y})$ï¼Œå³ç½‘ç»œå¯¹é¢„æµ‹æ ‡ç­¾$\hat{y}$çš„è¾“å‡ºæ—¶ï¼Œæ¢¯åº¦ä¸ºæ­£ã€‚
   5. å…¶ä»–æƒ…å†µä¸‹ï¼Œæ¢¯åº¦ä¸ºé›¶ã€‚


### Adversarial Transformation Networks (ATNs)

é€šè¿‡å¤šä¸ªå‰å‘ç¥ç»ç½‘ç»œæ¥ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œå¯ç”¨äºæ”»å‡»ä¸€ä¸ªæˆ–å¤šä¸ªç½‘ç»œã€‚

è¯¥ç®—æ³•é€šè¿‡æœ€å°åŒ–ä¸€ä¸ªè”åˆæŸå¤±å‡½æ•°æ¥ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œè¯¥æŸå¤±å‡½æ•°æœ‰ä¸¤ä¸ªéƒ¨åˆ†ï¼š

1. ç¬¬ä¸€éƒ¨åˆ†ä½¿å¯¹æŠ—æ ·æœ¬å’ŒåŸå§‹å›¾åƒä¿æŒç›¸ä¼¼ï¼›
2. ç¬¬äºŒéƒ¨åˆ†ä½¿å¯¹æŠ—æ ·æœ¬è¢«é”™è¯¯åˆ†ç±»ã€‚
